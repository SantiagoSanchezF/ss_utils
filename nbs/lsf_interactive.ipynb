{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c334a3e0-ec5a-4fa0-902f-14363b3c055d",
   "metadata": {},
   "source": [
    "# LSF interactive\n",
    "\n",
    "> send LSF jobs and colect results in interactive sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd1073-2be6-4c07-9c69-3364ae18a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp lsf_interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6d11eb-e0e9-4a6f-94bd-08ab16bd5d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878226e-3d9a-40ff-b66c-25104440a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "\n",
    "from fastcore.script import call_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9967142-fe83-4845-a841-1c6695ae0357",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ss_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#|notest\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mss_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ss_utils'"
     ]
    }
   ],
   "source": [
    "#|notest\n",
    "# from ss_utils.generic import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44d9eb4-0a4a-41d1-ac91-0e24c6792173",
   "metadata": {},
   "source": [
    "## Generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9163309-db5b-43dd-b46d-55f5b9e9af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e41bda6-4a60-440d-9659-e0a7b50d9f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_pickle(pickle_file):\n",
    "    \"read a pickle file\"\n",
    "    if os.path.isfile(pickle_file) == False:\n",
    "        return\n",
    "    with open(pickle_file, \"rb\") as h:\n",
    "        result = pickle.load(h)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65da0de7-9b8e-4485-a8be-7eb64781fbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def dump_pickle(obj,pickle_file,protocol=3):\n",
    "    \"dump an object to pickle file\"\n",
    "    with open(\"pickle_file\", \"wb\") as h:\n",
    "        pickle.dump(obj, h, protocol=protocol)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e9eac4-ee7d-4ffa-aa12-575007849278",
   "metadata": {},
   "source": [
    "## Individual job handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aead1af8-1265-4024-b62c-7886fac7e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class lsf_job:\n",
    "    def __init__(\n",
    "        self,\n",
    "        bsub_args,\n",
    "        job_name,\n",
    "        output_file,\n",
    "        args,\n",
    "        job_id=None,\n",
    "        status=\"unsend\",\n",
    "        iteration=1,\n",
    "        iteration_lim=3,\n",
    "        exit_code=None,\n",
    "        read_func=None\n",
    "    ):\n",
    "        self.bsub_args = bsub_args\n",
    "        self.job_name = job_name\n",
    "        self.output_file = output_file\n",
    "        self.args = args\n",
    "        self.job_id = job_id\n",
    "        self.stat = status\n",
    "        self.iteration = iteration\n",
    "        self.iter_lim = iteration_lim\n",
    "        self.result = None\n",
    "        self.stderr = None\n",
    "        self.stdout = None\n",
    "        self.exit_code = exit_code\n",
    "        self.read_func = read_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec1cbbe-025e-4de3-9f79-6ecde1e1a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def submit(tup):\n",
    "    bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func = tup\n",
    "    cmd = \"bsub -q {} -M {} -n {} -J {} -o {} -e {} {} {} {}\".format(\n",
    "        bsub_args[\"queue\"],\n",
    "        int(bsub_args[\"mem\"] + (bsub_args[\"mem\"] * (iteration - 1))),\n",
    "        bsub_args[\"n_cpus\"],\n",
    "        job_name,\n",
    "        f'{bsub_args[\"output_dir\"]}/{job_name}.stdout',\n",
    "        f'{bsub_args[\"output_dir\"]}/{job_name}.stderr',\n",
    "        bsub_args[\"interpreter\"],\n",
    "        bsub_args[\"script\"],\n",
    "        \" \".join(map(str, args)),\n",
    "    )\n",
    "    process = subprocess.Popen(\n",
    "        cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
    "    )\n",
    "    output, err = process.communicate()\n",
    "    \n",
    "    if err != b\"\":\n",
    "        # raise ValueError(\"error in sending job\")\n",
    "        stat = \"unsend\"\n",
    "    else:\n",
    "        job_id = re.split(\"[<>]\", output.decode())[1]\n",
    "        exit_code = None\n",
    "        stat = \"send\"\n",
    "    return bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8c1e27-4b61-49bb-8494-439b74cf755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def update_status(tup):\n",
    "    bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func = tup\n",
    "    if stat == \"unsend\":\n",
    "        bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func = submit((bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func))\n",
    "        return bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func\n",
    "    if stat == \"done\":\n",
    "        return None\n",
    "    cmd = f\"bjobs {job_id}\"\n",
    "    process = subprocess.Popen(\n",
    "        cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
    "    )\n",
    "    output, err = process.communicate()\n",
    "    stat = (\n",
    "        dict(list(zip(*[x.split() for x in output.decode().split(\"\\n\")][:2])))\n",
    "        .get(\"STAT\",\"EXIT\")[:4]\n",
    "        .lower()\n",
    "    )\n",
    "    if stat in [None, \"exit\"]:\n",
    "        stdout_file = f'{bsub_args[\"output_dir\"]}/{job_name}.stdout'\n",
    "        if os.path.isfile(stdout_file) == False:\n",
    "            stat = False\n",
    "            return\n",
    "        with open(stdout_file) as h:\n",
    "            rev = list(reversed(list(h)))\n",
    "            for ix, x in enumerate(rev):\n",
    "                if \"Sender\" in x:\n",
    "                    stat = rev[ix - 1].split()[-1][:4].lower()\n",
    "                    break\n",
    "    if stat == \"done\":\n",
    "        bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func = get_result((bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func))\n",
    "    if stat == \"exit\":\n",
    "        for ix, x in enumerate(rev):\n",
    "            if \"Exited with exit code\" in x:\n",
    "                exit_code = int(float(x.split()[-1]))\n",
    "                if (\n",
    "                    \"TERM_MEMLIMIT\" in rev[ix + 1]\n",
    "                ):  # exit_code == 1 and iter < iter_lim:\n",
    "                    bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func = submit((bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func))\n",
    "                    iteration += 1\n",
    "                    stat = None\n",
    "                break\n",
    "    if exit_code not in [None]:\n",
    "        stderr_file = f'{bsub_args[\"output_dir\"]}/{job_name}.stderr'\n",
    "        if os.path.isfile(stderr_file) == False:\n",
    "            stderr = False\n",
    "            return\n",
    "        with open(stderr_file) as h:\n",
    "            stderr = list(h)\n",
    "    return bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d18120a-5403-44e5-85c7-9060b54d4090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_result(tup):\n",
    "    bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func = tup\n",
    "    result = None\n",
    "    if read_func==None:\n",
    "        pickle_file = output_file\n",
    "        if os.path.isfile(pickle_file) == False:\n",
    "            stat = False\n",
    "            return output_file,stat,result\n",
    "        with open(pickle_file, \"rb\") as h:\n",
    "            result = pickle.load(h)\n",
    "    else:\n",
    "        result = read_func(output_file)\n",
    "    return bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc0d358-08f9-4b34-8fd5-9ef612a1852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LSF:\n",
    "    \" send a series of lsf jobs, collect the objects, and clean. Output objects must be a pickle \"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        script,\n",
    "        jobs,\n",
    "        interpreter=\"python3\",\n",
    "        monitor_file=\"/homes/fragoso/generic_monitor_file.txt\",\n",
    "        output_dir=\"/hps/nobackup/rdf/metagenomics/research-team/santiago/.std.generic\",\n",
    "        mem=1000,\n",
    "        n_cpus=1,\n",
    "        queue=\"research\",\n",
    "        n_jobs =1,\n",
    "    ):\n",
    "        self.monitor_file = monitor_file\n",
    "        self.bsub_args = {\n",
    "            \"script\": script,\n",
    "            \"interpreter\": interpreter,\n",
    "            \"output_dir\": output_dir,\n",
    "            \"mem\": mem,\n",
    "            \"n_cpus\": n_cpus,\n",
    "            \"queue\": queue,\n",
    "        }\n",
    "        self.n_jobs = n_jobs\n",
    "        os.mkdir(output_dir)\n",
    "        self.jobs_ = jobs\n",
    "        self.jobs = {job_name:lsf_job(self.bsub_args, job_name, output_file, args)\n",
    "            for job_name, output_file, args in self.jobs_\n",
    "                    }\n",
    "\n",
    "    def update(self,read_func=None):\n",
    "        \n",
    "        chunk = [(j.bsub_args, j.job_name, j.output_file, j.args, j.job_id, j.stat, j.iteration, j.iter_lim, j.result, j.stderr, j.stdout, j.exit_code, j.read_func)\n",
    "                    for k,j in self.jobs.items() if j.stat!=\"done\"]\n",
    "        print(f\"processing {len(chunk)} jobs\")\n",
    "        step = 0\n",
    "        size = 500\n",
    "        while step*size<len(chunk):\n",
    "            with multiprocessing.Pool(self.n_jobs) as pool:\n",
    "                res = pool.map(update_status,chunk[step*size:(step+1)*size])\n",
    "            for bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func in res:\n",
    "                self.jobs[job_name].bsub_args = bsub_args\n",
    "                self.jobs[job_name].job_name = job_name\n",
    "                self.jobs[job_name].output_file = output_file\n",
    "                self.jobs[job_name].args = args\n",
    "                self.jobs[job_name].job_id = job_id\n",
    "                self.jobs[job_name].stat = stat\n",
    "                self.jobs[job_name].iteration = iteration\n",
    "                self.jobs[job_name].iter_lim = iter_lim\n",
    "                self.jobs[job_name].result = result\n",
    "                self.jobs[job_name].stderr = stderr\n",
    "                self.jobs[job_name].stdout = stdout\n",
    "                self.jobs[job_name].exit_code = exit_code\n",
    "                self.jobs[job_name].read_func = read_func\n",
    "                \n",
    "            print(f\"step {step} done\")\n",
    "            step += 1\n",
    "            del res\n",
    "                \n",
    "        print(Counter([job.stat for _,job in self.jobs.items()]))\n",
    "\n",
    "    def clean(self):\n",
    "        shutil.rmtree(self.bsub_args[\"output_dir\"])\n",
    "\n",
    "    def retake(self, code):\n",
    "        \" keep gathering results after the original kernel is dead \"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ff7318-02b0-478a-8926-91aa4e9c5a39",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41d2eb5-19c6-4966-9f24-82cea5ade2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /homes/fragoso/Downloads/test.py\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "%%writefile /homes/fragoso/Downloads/test.py\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "outf, v = sys.argv[1:]\n",
    "\n",
    "a = {\"hello\": v}\n",
    "\n",
    "with open(outf, \"wb\") as handle:\n",
    "    pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a6447-0fca-41e8-9c4a-42452015713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\" this is the jobs array :: format.. job_name, output_file, args \"\n",
    "outputd = f'/homes/fragoso/Downloads/this{iitt}'\n",
    "iitt+=1\n",
    "jobs_arr = [\n",
    "    (\n",
    "        str(x),\n",
    "        f'{outputd}/{x}.pickle',\n",
    "        [f'{outputd}/{x}.pickle', x],\n",
    "    )\n",
    "    for x in range(10)\n",
    "]\n",
    "\n",
    "jobs = LSF(\"/homes/fragoso/Downloads/test.py\",jobs_arr,n_jobs=4,output_dir=outputd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44802c-bbbb-4bf3-acce-7355a518792c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0 jobs\n",
      "Counter({'done': 4})\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "jobs.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447ce7c2-b312-4089-8e43-4a1974aedadc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3abed4-d974-440c-b218-c083864665ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e8931-cf51-4660-ba31-e92aed002e10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
